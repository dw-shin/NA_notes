{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This lecture note is based on \"An Introduction to NUMERICAL ANALYSIS\" (2nd Edition) <br> by Kendall E. Atkinson\n",
    "\n",
    "\n",
    "# Chapter 2. ROOTFINDING FOR NONLINEAR EQUATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.10 Systems of Nonlinear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For simplicity of presenation and ease of understanding, the theory is presented for only two equations:\n",
    "\n",
    "$$ f_1(x_1,x_2) = 0 \\qquad f_2(x_1,x_2) = 0$$\n",
    "<br>\n",
    "\n",
    "Consider the solution in vector notation:\n",
    "\n",
    "$$\\boldsymbol{f}(\\boldsymbol{x}) = \\boldsymbol{0} \\qquad \\boldsymbol{x} = \\left[\\begin{array}{c} x_1\\\\x_2\\end{array}\\right] \\qquad  {f}(\\boldsymbol{x})= \\left[\\begin{array}{c} f_1(x_1,x_2)\\\\f_2(x_1,x_2)\\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fixed-point theory\n",
    "\n",
    "Assume that the rootfinding problem has been reformulated in an equivalent form as\n",
    "\n",
    "$$ x_1 = g_1(x_1,x_2) \\qquad x_2 = g_2(x_1,x_2)$$\n",
    "\n",
    "Denote its solution by\n",
    "\n",
    "$$\\boldsymbol{\\alpha} = \\left[\\begin{array}{c} \\alpha_1 \\\\ \\alpha_2\\end{array}\\right]$$\n",
    "\n",
    "We study the fixed-point iteration\n",
    "\n",
    "$$\\boldsymbol{x}_{n+1} = \\boldsymbol{g}(\\boldsymbol{x}_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "**Theorem 2.9** &emsp; Let $D\\in \\mathbb{R}^2$ be a closed, bounded, and convex set in the plane. Assume that the components of $\\boldsymbol{g}(\\boldsymbol{x})$ are continuously differentiable at all points of $D$, and further assume\n",
    "\n",
    "1. $g(D) \\subset D$,\n",
    "\n",
    "2. $\\displaystyle \\lambda \\equiv \\max_{x\\in D}\\|G(\\boldsymbol{x})|\\|_\\infty < 1$\n",
    "\n",
    "Then\n",
    "\n",
    "(a) $\\boldsymbol{x} = \\boldsymbol{g}(\\boldsymbol{x})$ has a unique solution $\\boldsymbol{\\alpha}\\in D$\n",
    "\n",
    "(b) For any initial point $\\boldsymbol{x}_0 \\in D$, the ieteration will converge in $D$ to $\\boldsymbol{\\alpha}$\n",
    "\n",
    "(c) $\\|\\boldsymbol{\\alpha} - \\boldsymbol{x}_{n+1}\\|_\\infty \\le (\\|G(\\boldsymbol{\\alpha})\\|_\\infty + \\epsilon_n)\\|\\boldsymbol{\\alpha} - \\boldsymbol{x}_n\\_\\infty$|\n",
    "\n",
    "&emsp; with $\\epsilon_n \\rightarrow 0$ as $n\\rightarrow \\infty$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "**Corollary 2.10** &emsp; Let $\\boldsymbol{\\alpha}$ be a fixed point of $\\boldsymbol{g}(\\boldsymbol{x})$, and assume components of $\\boldsymbol{g}(\\boldsymbol{x})$ are continuously differentiable in some neighborhood about $\\boldsymbol{\\alpha}$.\n",
    "Further assume\n",
    "\n",
    "$$\\|G(\\boldsymbol{\\alpha})\\|_\\infty < 1.$$\n",
    "\n",
    "Then for $\\boldsymbol{x}_0$ chosen sufficiently close to $\\boldsymbol{\\alpha}$, the iteration $\\boldsymbol{x}_{n+1} - \\boldsymbol{g}(\\boldsymbol{x}_n)$ will converge to $\\boldsymbol{\\alpha}$, and the results of Theorem 2.9 will be valid on some closed, bounded, convex region about $\\boldsymbol{\\alpha}$.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.11 Newton's Method for Nonlinear Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Apply Taylor's theorem for functions of two variables to each of the equations $f_i(x_1,x_2) = 0$, expanding $f_i(\\boldsymbol{\\alpha})$ about $\\boldsymbol{x}_0$: for $i = 1,2$\n",
    "\n",
    "\\begin{align}\n",
    "0 = f_i(\\boldsymbol{\\alpha}) &= f(\\boldsymbol{x}_0) + (\\alpha_1 - x_{1,0})\\frac{\\partial f_i(\\boldsymbol{x}_0)}{\\partial x_1} + (\\alpha_2 - x_{2,0})\\frac{\\partial f_i(\\boldsymbol{x}_0)}{\\partial x_2} \\\\\n",
    "&\\qquad + \\frac{1}{2}\\left[(\\alpha_1 - x_{1,0})\\frac{\\partial }{\\partial x_1} + (\\alpha_2 - x_{2,0})\\frac{\\partial }{\\partial x_2}\\right]^2 f_i(\\boldsymbol{\\xi}^{(i)})\n",
    "\\end{align}\n",
    "\n",
    "with $\\boldsymbol{\\xi}^{(i)}$ on the line segment joining $\\boldsymbol{x}_0$ and $\\boldsymbol{\\alpha}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we drop the second-order terms (under the assumption $\\boldsymbol{\\alpha} - \\boldsymbol{x}_0$ is small), we obtain the approximation\n",
    "\n",
    "\\begin{align}\n",
    "0 &\\approx f_1(\\boldsymbol{x}_0) + (\\alpha_1 - x_{0})\\frac{\\partial f_1(\\boldsymbol{x}_0)}{\\partial x} + (\\alpha_2 - y_{0})\\frac{\\partial f_1(\\boldsymbol{x}_0)}{\\partial y} \\\\\n",
    "0 &\\approx f_2(\\boldsymbol{x}_0) + (\\alpha_1 - x_{0})\\frac{\\partial f_2(\\boldsymbol{x}_0)}{\\partial x} + (\\alpha_2 - y_{0})\\frac{\\partial f_2(\\boldsymbol{x}_0)}{\\partial y} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In matrix form,\n",
    "$$0 \\approx \\boldsymbol{f}(\\boldsymbol{x}_0) + F(\\boldsymbol{x}_0)(\\boldsymbol{\\alpha}-\\boldsymbol{x}_0)$$\n",
    "with $F(\\boldsymbol{x}_0)$ the Jacobian matrix of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Solving for $\\boldsymbol{\\alpha}$,\n",
    "$$\\boldsymbol{\\alpha} \\approx \\boldsymbol{x}_0 - F(\\boldsymbol{x}_0)^{-1} \\boldsymbol{f}(\\boldsymbol{x}_0) \\equiv \\boldsymbol{x}_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton's method for solving nonlinear systems\n",
    "\n",
    "$$\\boldsymbol{x}_{n+1} = \\boldsymbol{x}_n - F(\\boldsymbol{x}_n)^{-1} \\boldsymbol{f}(\\boldsymbol{x}_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In practice, we solve\n",
    "\\begin{align}\n",
    "F(\\boldsymbol{x}_n)\\boldsymbol{\\delta}_{n+1} &= -\\boldsymbol{f}(\\boldsymbol{x}_n) \\\\\n",
    "\\boldsymbol{x}_{n+1} &= \\boldsymbol{x}_n + \\boldsymbol{\\delta}_{n+1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convergence analysis\n",
    "\n",
    "Corollary 2.10 implies that $\\boldsymbol{x}_n$ converges to $\\boldsymbol{\\alpha}$, provided $\\boldsymbol{x}_0$ is chosen sufficiently close to $\\boldsymbol{\\alpha}$. <br><br>\n",
    "\n",
    "In addition, it can be shown that the iteration is quadratic.\n",
    "$$\\|\\boldsymbol{\\alpha} - \\boldsymbol{x}_{n+1}\\|_\\infty \\le B\\|\\boldsymbol{\\alpha} - \\boldsymbol{x}_{n}\\|^2_\\infty \\qquad n\\ge 0$$\n",
    "for some constant $B>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.12 Unconstrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Optimization: finding the maximum or minimum\n",
    "\n",
    "- Consider to find the local minima $\\alpha$ of a continuous function\n",
    "$$f(x_1,\\cdots,x_m)$$\n",
    "\n",
    "- Unconstrained optimization: no limits on $x_1,\\cdots,x_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If $\\alpha$ is the local minimum of $f$, then we have\n",
    "$$\\frac{\\partial f}{\\partial x_i}(\\alpha) = 0,\\qquad i = 1,\\cdots,m$$\n",
    "<br>\n",
    "\n",
    "Thus we solve the nonlinear system\n",
    "$$\\frac{\\partial f}{\\partial x_i}(\\boldsymbol{x}) = 0, \\quad i = 1,\\cdots, m \\qquad \\Rightarrow \\qquad F(\\boldsymbol{x}) \\equiv \\nabla f(\\boldsymbol{x}) = \\left[\\begin{array}{c}\\partial_{x_1} f \\\\ \\vdots \\\\ \\partial_{x_m} f\\end{array}\\right] = \\boldsymbol{0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton's method\n",
    "\n",
    "Newton's method leads to\n",
    "\n",
    "$$\\boldsymbol{x}_{n+1} = \\boldsymbol{x}_n - H(\\boldsymbol{x_n})^{-1} F(\\boldsymbol{x}_n),$$\n",
    "\n",
    "where $H(\\boldsymbol{x})$ is the Hessian matrix of $f$\n",
    "\n",
    "$$(H(\\boldsymbol{x}))_{ij} = \\frac{\\partial^2 f(\\boldsymbol{x})}{\\partial x_i \\partial x_j}, \\quad 1\\le i,\\ j \\le m.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Descent method\n",
    "\n",
    "Most of descent methods have two-step iteration process\n",
    "\n",
    "- **Step 1**: At $\\boldsymbol{x}_n$, pick a direction $\\boldsymbol{d}_n$ such that $f(\\boldsymbol{x})$ will decrease as $\\boldsymbol{x}$ moves away from $\\boldsymbol{x_n}$ in the direction $\\boldsymbol{d}_n$.\n",
    "<br>\n",
    "- **Step 2**: Let $\\boldsymbol{x}_{n+1} = \\boldsymbol{x}_n + s\\boldsymbol{d}_n$ with $s$ chosen to minimize\n",
    "$$\\varphi(s) = f(\\boldsymbol{x}_n + s\\boldsymbol{d}_n),\\quad s\\ge 0 \\qquad \\Rightarrow \\qquad f(\\boldsymbol{x}_{n+1}) < f(\\boldsymbol{x}_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Descent method\n",
    "\n",
    "- Steepest descent method($\\boldsymbol{d}_n = -\\nabla f(\\boldsymbol{x}_n)$): usually good near $\\boldsymbol{x}_n$ but generally very slow\n",
    "<br>\n",
    "\n",
    "- Quasi-Newton method(use approximations of $H(\\boldsymbol{x}_n)$ or $H(\\boldsymbol{x}_n)^{-1}$): Davidon-Fletcher-Powel method, Broyden method, and so on.\n",
    "<br>\n",
    "\n",
    "- Conjugate gradient method: uses a generalization of the idea of an orthogonal basis for a vector space to generate the direction $\\boldsymbol{d}_n$ with the directions related in an optimal way to the function $f(\\boldsymbol{x})$ being minimized."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
